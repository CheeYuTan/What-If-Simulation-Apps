{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa23e36c-ea57-418d-8e92-d1774db4321f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Inference Job Configuration Guide\n",
    "\n",
    "## Overview\n",
    "This configuration sets up a Databricks job for batch inference processing of wine quality predictions. The job is designed to process CSV files containing wine features and generate predictions with SHAP explanations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b3fc68-98c9-442f-bc21-7f681e4c6dff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job Configuration\n",
    "\n",
    "### Basic Information\n",
    "- **Job Name**: `Batch-Inference-Job`\n",
    "- **Source**: WORKSPACE\n",
    "\n",
    "### Cluster Configuration\n",
    "- **Spark Version**: 15.4.x-scala2.12\n",
    "- **Node Type**: Standard_D4ds_v5\n",
    "- **Runtime Engine**: STANDARD\n",
    "- **Cluster Type**: CLASSIC_PREVIEW\n",
    "- **ML Runtime**: Enabled\n",
    "- **Single Node**: Yes\n",
    "- **Data Security Mode**: DATA_SECURITY_MODE_DEDICATED\n",
    "\n",
    "![Job Compute Configuration](screenshot/JobCompute.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e36bba-d495-4b65-b109-b309b08cfaba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Job Parameters\n",
    "The job accepts the following parameters:\n",
    "1. `catalog_name`: Target catalog for data storage\n",
    "2. `schema_name`: Target schema within the catalog\n",
    "3. `file_name`: Name of the input CSV file to process\n",
    "4. `folder_name`: Name of the input CSV folder (to segregate differnet users)\n",
    "\n",
    "![Job Parameters Configuration](screenshot/JobParameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61a5053c-2ce1-4877-8065-1c5698f0176e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deployment Instructions\n",
    "\n",
    "1. **Create Job**\n",
    "   - Navigate to Databricks Workspace\n",
    "   - Go to Jobs â†’ Create Job\n",
    "   - Select \"Notebook\" as the task type\n",
    "\n",
    "2. **Configure Job Settings**\n",
    "   - Set the job name to `Batch-Inference-Job`\n",
    "   - Specify the notebook path\n",
    "\n",
    "3. **Set Up Cluster**\n",
    "   - Create a new cluster with the specified configuration\n",
    "   - Ensure ML runtime is enabled\n",
    "   - Set up the environment variables\n",
    "\n",
    "4. **Configure Parameters**\n",
    "   - Add the four required parameters:\n",
    "     - `catalog_name`\n",
    "     - `schema_name`\n",
    "     - `file_name`\n",
    "     - `folder_name`\n",
    "   - Keep them as empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c917b256-2bb5-43e3-841b-dee2651f35d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\")\n",
    "dbutils.widgets.text(\"folder_name\", \"\")\n",
    "dbutils.widgets.text(\"file_name\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c51099-6f13-48de-a4df-33457eb674a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Training Wine Quality Model with SHAP Explanations\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "folder_name = dbutils.widgets.get(\"folder_name\")\n",
    "volume_upload_path = \"batch_inference_upload\"\n",
    "volume_download_path = \"batch_inference_download\"\n",
    "file_name = dbutils.widgets.get(\"file_name\")\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "query = f\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# Create upload and download volumes within the catalog and schema\n",
    "query_upload_volume = f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_upload_path}\n",
    "\"\"\"\n",
    "spark.sql(query_upload_volume)\n",
    "\n",
    "query_download_volume = f\"\"\"\n",
    "CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_download_path}\n",
    "\"\"\"\n",
    "spark.sql(query_download_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9caa4265-699c-4156-8248-7bac54c8b4b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "logged_model = f'models:/{catalog_name}.{schema_name}.wine_quality_with_shap@champion'\n",
    "\n",
    "# Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1440b5-09ce-439f-9346-293013d0afbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(f\"/Volumes/{catalog_name}/{schema_name}/{volume_upload_path}/{folder_name}/{file_name}\").toPandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0988bcb-8ae0-4825-8903-1339b63e3cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predict on a Pandas DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "model_output = loaded_model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019bc34a-3015-4b49-9378-4a0389eb20b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_batch_prediction(df, model_output):\n",
    "    # Create a results DataFrame\n",
    "    results = df.copy()\n",
    "    \n",
    "    # Add prediction column\n",
    "    results['prediction'] = model_output['predictions']\n",
    "    \n",
    "    # Add SHAP values as separate columns\n",
    "    for i, feature in enumerate(model_output['feature_names']):\n",
    "        results[f'shap_{feature}'] = [shap[i] for shap in model_output['shap_values']]\n",
    "  \n",
    "    \n",
    "    # Reorder columns to match real-time format\n",
    "    feature_cols = [col for col in results.columns if col not in ['row_id', 'prediction'] and not col.startswith('shap_')]\n",
    "    shap_cols = [col for col in results.columns if col.startswith('shap_')]\n",
    "    \n",
    "    final_cols = feature_cols + ['prediction'] + shap_cols\n",
    "    \n",
    "    return results[final_cols]\n",
    "\n",
    "results_df = process_batch_prediction(df, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566d798b-719b-4267-a0b8-ac366a7a72a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(results_df).createOrReplaceTempView(\"prediction_results\")\n",
    "\n",
    "explanation_query = f\"\"\"\n",
    "SELECT \n",
    "    *,\n",
    "    ai_query(\n",
    "        '{catalog_name}-{schema_name}-wine_quality_interpreter',\n",
    "        CONCAT(\n",
    "            'Analyze these wine quality prediction features and SHAP values: ',\n",
    "            to_json(named_struct(\n",
    "                'features', named_struct(\n",
    "                    'fixed_acidity', fixed_acidity,\n",
    "                    'volatile_acidity', volatile_acidity,\n",
    "                    'citric_acid', citric_acid,\n",
    "                    'residual_sugar', residual_sugar,\n",
    "                    'chlorides', chlorides,\n",
    "                    'free_sulfur_dioxide', free_sulfur_dioxide,\n",
    "                    'total_sulfur_dioxide', total_sulfur_dioxide,\n",
    "                    'density', density,\n",
    "                    'pH', pH,\n",
    "                    'sulphates', sulphates,\n",
    "                    'alcohol', alcohol,\n",
    "                    'is_red', is_red\n",
    "                ),\n",
    "                'shap_values', named_struct(\n",
    "                    'shap_fixed_acidity', shap_fixed_acidity,\n",
    "                    'shap_volatile_acidity', shap_volatile_acidity,\n",
    "                    'shap_citric_acid', shap_citric_acid,\n",
    "                    'shap_residual_sugar', shap_residual_sugar,\n",
    "                    'shap_chlorides', shap_chlorides,\n",
    "                    'shap_free_sulfur_dioxide', shap_free_sulfur_dioxide,\n",
    "                    'shap_total_sulfur_dioxide', shap_total_sulfur_dioxide,\n",
    "                    'shap_density', shap_density,\n",
    "                    'shap_pH', shap_pH,\n",
    "                    'shap_sulphates', shap_sulphates,\n",
    "                    'shap_alcohol', shap_alcohol,\n",
    "                    'shap_is_red', shap_is_red\n",
    "                ),\n",
    "                'prediction', prediction\n",
    "            ))\n",
    "        )\n",
    "    ) as explanation\n",
    "FROM prediction_results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68068248-5e43-4616-937a-e7c25199b53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute the query\n",
    "explanations_df = spark.sql(explanation_query)\n",
    "\n",
    "# Save the final results\n",
    "output_dir = f\"/Volumes/{catalog_name}/{schema_name}/{volume_download_path}/{folder_name}\"\n",
    "dbutils.fs.mkdirs(output_dir)\n",
    "output_path = f\"{output_dir}/results_with_explanations_{file_name}\"\n",
    "explanations_df.toPandas().to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Batch-Inference-Job",
   "widgets": {
    "catalog_name": {
     "currentValue": "steventan",
     "nuid": "91643db9-17b6-43d7-b8fc-e4d057e789cf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "file_name": {
     "currentValue": "batch-inference-data.csv",
     "nuid": "b9cf3a95-e2bc-43ee-9252-07f8e121320c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "what_if_simulation_apps",
     "nuid": "7132e7fb-08f0-4697-b0f2-8fe01a96698b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_download_path": {
     "currentValue": "",
     "nuid": "3eae5d35-98b6-4b80-bf1a-b757c2645dd8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "volume_download_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "volume_download_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "volume_path": {
     "currentValue": "batch_inference_upload",
     "nuid": "feaea1ac-b55d-4e69-bb43-dd7ebd739fff",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "volume_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_upload_path": {
     "currentValue": "",
     "nuid": "164936f8-f7c9-4809-a929-d77e327073d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "volume_upload_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "volume_upload_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
